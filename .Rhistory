abline(a = 0, b = 1, lty = 3)
performance(c.pred_val, measure = "auc")@y.values[1]
performance(c.pred_val, measure = "auc")@y.values[[1]]
performance(c.pred_val, measure = "auc")@y.values[[1]][1]
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + performance(c.pred_val, measure = "auc")@y.values[[1]]),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
performance(c.pred_val, measure = "auc")@y.values[[1]][1]
performance(c.pred_val, measure = "auc")@y.values[[1]]
as.numeric(performance(c.pred_val, measure = "auc")@y.values[[1]])
c.auroc<-as.numeric(performance(c.pred_val, measure = "auc")@y.values[[1]])
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = print("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = 0.8288112"),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
#load data
df <- read.csv("https://raw.githubusercontent.com/felicitimilne/bluefall2hwteam17/main/hrl_load_metered.csv")
head(df)
df2 <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/blob/main/hrl_load_metered%20-%20test1.csv")
head(validation)
head(df2)
df2 <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test1.csv")
head(df2)
rbind(df,df2)
df3 <- rbind(df,df2)
View(df3)
View(df2)
validation <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test2.csv")
#get rid of useless variables
df3 <- df[,c(1,6)]
validation <- validation[,c(1,6)]
library(dplyr)
library(ggfortify)
library(lubridate)
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
#load data
df <- read.csv("https://raw.githubusercontent.com/felicitimilne/bluefall2hwteam17/main/hrl_load_metered.csv")
head(df)
df2 <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test1.csv")
head(df2)
df3 <- rbind(df,df2)
validation <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test2.csv")
#get rid of useless variables
df3 <- df[,c(1,6)]
validation <- validation[,c(1,6)]
#Change variable to a date time object
df3$datetime_beginning_ept <- mdy_hm(df3$datetime_beginning_ept, tz = Sys.timezone())
validation$datetime_beginning_ept <- mdy_hm(validation$datetime_beginning_ept, tz = Sys.timezone())
#Impute the average of previous and next observation to fix the zeros for DLS
df3[c(5280:5290),]
df3[5283,2] <- 904.2965
df3[c(14180:14190),]
df3[14187,2] <- 844.047
# create time series object
energy <- ts(df3[,2], start = 2019, frequency = 24) # frequency = 24 hours * 365.25 days in a year
# autoplot
autoplot(energy) +
ggtitle("Energy Usage") +
xlab("Time") +
ylab("Energy")
#decomposition plot
decomp_stl <- stl(energy, s.window = 7)
plot(decomp_stl)
autoplot(decomp_stl)
#subseries plot that plots the averages of the seasons
ggsubseriesplot(energy)
prophet.data <- data.frame(ds = df3$datetime_beginning_ept, y = df3$mw)
View(prophet.data)
prophet.data <- data.frame(ds = df3$datetime_beginning_ept, y = df3$mw)
Prof <- prophet()
Prof <- add_country_holidays(Prof, "US")
Prof <- add_seasonality(Prof, name='weekly', period=7, fourier.order=6)
Prof <- fit.prophet(Prof, prophet.data)
prophet.data <- data.frame(ds = df3$datetime_beginning_ept, y = df3$mw)
Prof <- prophet()
Prof <- add_country_holidays(Prof, "US")
Prof <- add_seasonality(Prof, name='daily', period=7, fourier.order=6)
Prof <- fit.prophet(Prof, prophet.data)
Prof <- prophet()
Prof <- add_country_holidays(Prof, "US")
Prof <- add_seasonality(Prof, name='monthly', period=30.5, fourier.order=6)
Prof <- fit.prophet(Prof, prophet.data)
forecast.data <- make_future_dataframe(Prof, periods = 128, freq = 'hour')
forecast.data <- make_future_dataframe(Prof, periods = 168, freq = 'hour')
plot(Prof, predict(Prof, forecast.data))
View(forecast.data)
View(prophet.data)
forecast.data <- make_future_dataframe(Prof, periods = 168, freq = 'hour')
View(forecast.data)
View(df3)
plot(Prof, forecast.data)
plot(forecast.data)
predict(Prof, forecast.data)$yhat
plot(forecast.data,predict(Prof, forecast.data))
plot(Prof, predict(Prof, forecast.data))
# Calculate prediction errors from forecast
Prophet.error <- validation - tail(predict(Prof, forecast.data)$yhat, 168)
Prophet.error <- validation - tail(predict(Prof, forecast.data)$yhat, 168)
# libraries
library(dplyr)
library(ggfortify)
library(lubridate)
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
Prophet.error <- validation - tail(predict(Prof, forecast.data)$yhat, 168)
Prophet.MAE <- mean(abs(Prophet.error))
Prophet.MAPE <- mean(abs(Prophet.error)/abs(test))*100
Prophet.MAE
Prophet.MAPE
View(Prophet.error)
Prophet.MAE <- mean(abs(Prophet.error$mw))
Prophet.MAPE <- mean(abs(Prophet.error$mw)/abs(test$mw))*100
Prophet.MAE
Prophet.MAPE
Prophet.MAE <- mean(abs(Prophet.error$mw))
Prophet.MAPE <- mean(abs(Prophet.error$mw)/abs(validation$mw))*100
Prophet.MAE
Prophet.MAPE
ta_metrics <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/yearly_sent_metrics.csv")
library(tidyverse)
library(ggplot2)
library(gghighlight)
ta_metrics <- ta_metrics %>% mutate(Avg.Anger = -1 * Avg.Anger, Avg.Disgust = -1 * Avg.Disgust,
Avg.Fear = -1 * Avg.Fear, Avg.Neg = -1 * Avg.Neg, Avg.Sadness = -1 * Avg.Sadness) %>%
rename(Neg.Anger = Avg.Anger, Neg.Disgust = Avg.Disgust, Neg.Fear = Avg.Fear, Neg.Negative = Avg.Neg, Neg.Sadness = Avg.Sadness,
Pos.Anticipation = Avg.Antic, Pos.Joy = Avg.Joy, Pos.Positive = Avg.Pos, Pos.Surprise = Avg.Surprise, Pos.Trust = Avg.Trust)
ggplot(data = ta_metrics) + geom_line(aes(x = Year, y = Avg.Exp.Valence), color = "#648fff") +
scale_x_continuous(breaks = seq(min(ta_metrics$Year),max(ta_metrics$Year),by=1)) +
geom_line(aes(x = Year, y = rep(0, nrow(ta_metrics))), color = "#ffb000", linetype = "dashed") +
geom_point(aes(x = 2019, y = Avg.Exp.Valence[14]), color = "#dc267f")
tam_avg <- data.frame()
for (i in 9:19) {
tam_avg <- rbind(tam_avg, c(colnames(ta_metrics)[i], mean(ta_metrics[,i]), "Overall Average"))
}
colnames(tam_avg) <- c("Metric", "Value", "Type")
tam_long <- ta_metrics %>% pivot_longer(cols = -1, names_to = "Metric")
tam_2019 <- tam_long %>% filter(X == 13) %>% dplyr::select(-X) %>%
mutate(Type = "2019") %>% rename(Value = value)
tam_2019 <- tam_2019[8:18,]
tam_2019 <- rbind(tam_2019, data.frame(tam_avg))
tam_2019 <- tam_2019 %>% mutate(Value = as.numeric(Value))
ggplot(data = tam_2019) + geom_bar(aes(x = Metric, y = Value , fill = Type), position = "dodge", stat = "identity") +
scale_x_discrete(labels = c("Valence", "Anger", "Disgust", "Fear", "Negativity", "Sadness", "Anticipation", "Joy", "Positivity", "Surprise", "Trust")) +
ylim(c(-0.2, 0.2)) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
tam_2020 <- tam_long %>% filter(X == 14) %>% dplyr::select(-X) %>%
mutate(Type = "2020") %>% rename(Value = value)
tam_2020 <- tam_2020[8:18,]
tam_2021 <- tam_long %>% filter(X == 15) %>% dplyr::select(-X) %>%
mutate(Type = "2021") %>% rename(Value = value)
tam_2021 <- tam_2021[8:18,]
tam_2019_20_21 <- rbind(tam_2019, tam_2020, tam_2021, data.frame(tam_avg))
tam_2019_20_21 <- tam_2019_20_21 %>% mutate(Value = as.numeric(Value))
ggplot(data = tam_2019_20_21) + geom_bar(aes(x = Metric, y = Value , fill = Type), position = "dodge", stat = "identity") +
geom_segment(aes(x = 1, xend = 1, y = -0.25, yend = 0.25), color = "#bbbbbb") +
scale_x_discrete(labels = c("Valence", "Anger", "Disgust", "Fear", "Negativity", "Sadness", "Anticipation", "Joy", "Positivity", "Surprise", "Trust")) +
scale_fill_manual(labels = c("2019", "2020", "2021", "Overall Average"), values = c("#648fff", "#ffb000", "#dc267f", "#888888")) +
ylim(c(-0.2, 0.2)) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5), legend.title = element_text(hjust = 0.5), plot.title = element_text(hjust = 0.5)) +
labs(x = "Emotion", y = "Average Value", title = "Late-Year Emotion Analysis")
from_2012 <- ta_metrics %>% filter(Year >= 2012)
from_2012_long <- from_2012 %>% mutate(Avg.Exp.Valence = Avg.Exp.Valence + 0.5) %>% dplyr::select(c(2, 19, 21:22)) %>% pivot_longer(cols = -1, names_to = "Metric")
to_2012 <- ta_metrics %>% filter(Year <= 2011)
to_2012_long <- to_2012 %>% mutate(Avg.Exp.Valence = Avg.Exp.Valence + 0.5) %>% dplyr::select(c(2, 19, 21)) %>% pivot_longer(cols = -1, names_to = "Metric")
long_sent <- ta_metrics %>% mutate(Avg.Exp.Valence = Avg.Exp.Valence + 0.5) %>% dplyr::select(c(2, 19, 21)) %>% pivot_longer(cols = -1, names_to = "Metric")
ggplot(data = long_sent, aes(x = Year, y = value, color = Metric)) + geom_line() +
scale_y_continuous(sec.axis = sec_axis(~.-0.5, name = "Expanded Valence")) +
theme(legend.title = element_text(hjust = 0.5), plot.title = element_text(hjust = 0.5)) +
scale_color_manual(labels = c("Expanded Valence", "Global Sentiment Indicator"), values = c("#648fff", "#ffb000")) +
labs(x = "Year", y = "Combined Sentiment Index", col = "Sentiment Metric", title = "Popular Song Sentiment vs Global Economic Sentiment")
ggplot(data = to_2012_long, aes(x = Year, y = value, color = Metric)) + geom_line() +
scale_y_continuous(sec.axis = sec_axis(~.-0.5, name = "Expanded Valence")) +
theme(legend.title = element_text(hjust = 0.5), plot.title = element_text(hjust = 0.5)) +
scale_color_manual(labels = c("Expanded Valence", "Global Sentiment Indicator"), values = c("#648fff", "#ffb000")) +
labs(x = "Year", y = "Combined Sentiment Index", col = "Sentiment Metric", title = "Popular Song Sentiment vs Global Economic Sentiment until 2012")
twitter_only <- from_2012_long %>% filter(Metric != "Avg.GS.Indicator")
ggplot(data = twitter_only, aes(x = Year, y = value, color = Metric)) + geom_line() +
scale_y_continuous(sec.axis = sec_axis(~.-0.5, name = "Expanded Valence")) +
theme(legend.title = element_text(hjust = 0.5), plot.title = element_text(hjust = 0.5)) +
scale_color_manual(labels = c("Expanded Valence", "Twitter Sentiment"), values = c("#648fff", "#dc267f")) +
labs(x = "Year", y = "Combined Sentiment Index", col = "Sentiment Metric", title = "Popular Song Sentiment vs Twitter Sentiment")
comb_metr_df <- data.frame(rbind(to_2012_long, twitter_only)) %>% mutate(Metric2 = ifelse(Metric == "Avg.Exp.Valence", Metric, "Sent.Index"))
ggplot(data = comb_metr_df, aes(x = Year, y = value, color = interaction(Metric, Metric2))) + geom_line() +
scale_y_continuous(sec.axis = sec_axis(~.-0.5, name = "Expanded Valence")) +
theme(legend.title = element_text(hjust = 0.5), plot.title = element_text(hjust = 0.5)) +
geom_segment(aes(x = 2011, xend = 2012, y = comb_metr_df["value"][12,] + 0.001, yend = comb_metr_df["value"][14,] + 0.001), color = "#888888", linetype = 2) +
scale_color_manual(labels = c("Expanded Valence", "Global Sentiment Indicator", "Twitter Sentiment"), values = c("#648fff", "#ffb000", "#dc267f")) +
labs(x = "Year", y = "Combined Sentiment Index", col = "Sentiment Metric", title = "Popular Song Sentiment vs Combined Global Sentiment")
ggplot(data = ta_metrics) + geom_line(aes(x = Year, y = Avg.Exp.Valence), color = "#648fff") +
scale_x_continuous(breaks = seq(min(ta_metrics$Year),max(ta_metrics$Year),by=1)) +
geom_line(aes(x = Year, y = rep(0, nrow(ta_metrics))), color = "#ffb000", linetype = "dashed") +
geom_point(aes(x = 2019, y = Avg.Exp.Valence[14]), color = "#dc267f") +
geom_hline(y = mean(Avg.Exp.Valence))
ggplot(data = ta_metrics) + geom_line(aes(x = Year, y = Avg.Exp.Valence), color = "#648fff") +
scale_x_continuous(breaks = seq(min(ta_metrics$Year),max(ta_metrics$Year),by=1)) +
geom_line(aes(x = Year, y = rep(0, nrow(ta_metrics))), color = "#ffb000", linetype = "dashed") +
geom_point(aes(x = 2019, y = Avg.Exp.Valence[14]), color = "#dc267f") +
geom_hline(y = mean(ta_metrics$Avg.Exp.Valence))
ggplot(data = ta_metrics) + geom_line(aes(x = Year, y = Avg.Exp.Valence), color = "#648fff") +
scale_x_continuous(breaks = seq(min(ta_metrics$Year),max(ta_metrics$Year),by=1)) +
geom_line(aes(x = Year, y = rep(0, nrow(ta_metrics))), color = "#ffb000", linetype = "dashed") +
geom_point(aes(x = 2019, y = Avg.Exp.Valence[14]), color = "#dc267f")
# code for time series 2 homework 1
# libraries
library(dplyr)
library(ggfortify)
library(lubridate)
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
#load data
df <- read.csv("https://raw.githubusercontent.com/felicitimilne/bluefall2hwteam17/main/hrl_load_metered.csv")
head(df)
df2 <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test1.csv")
head(df2)
df3 <- rbind(df,df2)
validation <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test2.csv")
#get rid of useless variables
df3 <- df[,c(1,6)]
validation <- validation[,c(1,6)]
#Change variable to a date time object
df3$datetime_beginning_ept <- mdy_hm(df3$datetime_beginning_ept, tz = Sys.timezone())
validation$datetime_beginning_ept <- mdy_hm(validation$datetime_beginning_ept, tz = Sys.timezone())
#Impute the average of previous and next observation to fix the zeros for DLS
df3[c(5280:5290),]
df3[5283,2] <- 904.2965
df3[c(14180:14190),]
df3[14187,2] <- 844.047
# create time series object
energy <- ts(df3[,2], start = 2019, frequency = 24) # frequency = 24 hours * 365.25 days in a year
# autoplot
autoplot(energy) +
ggtitle("Energy Usage") +
xlab("Time") +
ylab("Energy")
#decomposition plot
decomp_stl <- stl(energy, s.window = 7)
plot(decomp_stl)
autoplot(decomp_stl)
#subseries plot that plots the averages of the seasons
ggsubseriesplot(energy)
# Prophet
prophet.data <- data.frame(ds = df3$datetime_beginning_ept, y = df3$mw)
Prof <- prophet()
Prof <- add_country_holidays(Prof, "US")
Prof <- add_seasonality(Prof, name='monthly', period=30.5, fourier.order=6)
Prof <- fit.prophet(Prof, prophet.data)
forecast.data <- make_future_dataframe(Prof, periods = 168, freq = 'hour')
#plot(Prof, predict(Prof, forecast.data))
# Calculate prediction errors from forecast
Prophet.error <- validation - tail(predict(Prof, forecast.data)$yhat, 168)
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error$mw))
Prophet.MAPE <- mean(abs(Prophet.error$mw)/abs(validation$mw))*100
Prophet.MAE
Prophet.MAPE
set.seed(476)
NN.Model <- nnetar(diff(training, 24), p = 1, P = 2)
set.seed(476)
NN.Model <- nnetar(diff(energy, 24), p = 1, P = 2)
NN.Forecast <- forecast::forecast(NN.Model, h = 168)
plot(NN.Forecast)
Pass.Forecast <- rep(NA, 168)
for(i in 1:168){
Pass.Forecast[i] <- energy[length(energy) - 168 + i] + NN.Forecast$mean[i]
}
Pass.Forecast <- ts(Pass.Forecast, start = 2021, frequency = 24)
# Calculate prediction errors from forecast
NN.error <- validation - Pass.Forecast
validation <- ts(validation[,2], start = 2021, frequency = 24)
# Calculate prediction errors from forecast
NN.error <- validation - Pass.Forecast
# Calculate prediction error statistics (MAE and MAPE)
NN.MAE <- mean(abs(NN.error))
NN.MAPE <- mean(abs(NN.error)/abs(test))*100
# Calculate prediction error statistics (MAE and MAPE)
NN.MAE <- mean(abs(NN.error))
NN.MAPE <- mean(abs(NN.error)/abs(validation))*100
NN.MAE
NN.MAPE
checkresiduals(NN.Model)
checkresiduals(Prof)
Prophet.error <- validation - tail(predict(Prof, forecast.data)$yhat, 168)
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error$mw))
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error$mw))
Prophet.MAE <- mean(abs(Prophet.error))
Prophet.MAPE <- mean(abs(Prophet.error)/abs(validation))*100
Prophet.MAE
Prophet.MAPE
NN.error <- validation - Pass.Forecast
# Calculate prediction error statistics (MAE and MAPE)
NN.MAE <- mean(abs(NN.error))
NN.MAPE <- mean(abs(NN.error)/abs(validation))*100
NN.MAE
NN.MAPE
HW <- hw(energy, seasonal = "additive", h = 168)
summary(HW)
HW <- hw(energy, seasonal = "additive", h = 168)
summary(HW)
# Calculate prediction errors from forecast
error=validation-HW$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE=mean(abs(error))
MAPE=mean(abs(error)/abs(validation))
#Plot the forecasts from the holt-winters model
autoplot(HW)+
autolayer(fitted(HW),series="Fitted")+ylab("US Steel Shipments")
# Calculate prediction errors from forecast
error=validation-HW$mean
# code for time series 2 homework 1
# libraries
library(dplyr)
library(ggfortify)
library(lubridate)
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
#load data
df <- read.csv("https://raw.githubusercontent.com/felicitimilne/bluefall2hwteam17/main/hrl_load_metered.csv")
head(df)
df2 <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test1.csv")
head(df2)
df3 <- rbind(df,df2)
validation <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test2.csv")
#get rid of useless variables
df3 <- df[,c(1,6)]
validation <- validation[,c(1,6)]
#Change variable to a date time object
df3$datetime_beginning_ept <- mdy_hm(df3$datetime_beginning_ept, tz = Sys.timezone())
validation$datetime_beginning_ept <- mdy_hm(validation$datetime_beginning_ept, tz = Sys.timezone())
#Impute the average of previous and next observation to fix the zeros for DLS
df3[c(5280:5290),]
df3[5283,2] <- 904.2965
df3[c(14180:14190),]
df3[14187,2] <- 844.047
# create time series object
energy <- ts(df3[,2], start = 2019, frequency = 24) # frequency = 24 hours * 365.25 days in a year
HW <- hw(energy, seasonal = "additive", h = 168)
summary(HW)
# Calculate prediction errors from forecast
error=validation-HW$mean
# Calculate prediction errors from forecast
error=validation$mw-HW$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE=mean(abs(error))
MAPE=mean(abs(error)/abs(validation$mw))
MAE
MAPE
Prophet.error <- validation$mw - tail(predict(Prof, forecast.data)$yhat, 168)
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error))
Prophet.MAPE <- mean(abs(Prophet.error)/abs(validation$mw))*100
Prophet.MAE
Prophet.MAPE
# Calculate prediction errors from forecast
NN.error <- validation_ts - Pass.Forecast
validation_ts <- ts(validation[,2], start = 2022, frequency = 24)
# Calculate prediction errors from forecast
NN.error <- validation_ts - Pass.Forecast
# Calculate prediction error statistics (MAE and MAPE)
NN.MAE <- mean(abs(NN.error))
NN.MAPE <- mean(abs(NN.error)/abs(validation_ts))*100
NN.MAE
NN.MAPE
Prophet.error <- validation$mw - tail(predict(Prof, forecast.data)$yhat, 168)
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error))
Prophet.MAPE <- mean(abs(Prophet.error)/abs(validation$mw))*100
Prophet.MAE
Prophet.MAPE
View(validation)
View(validation)
View(validation)
test <- read.csv("https://github.com/felicitimilne/bluefall2hwteam17/raw/main/hrl_load_metered%20-%20test3.csv")
test <- test[,c(1,6)]
test$datetime_beginning_ept <- mdy_hm(test$datetime_beginning_ept, tz = Sys.timezone())
new_val <- rbind(df3, validation)
##Test
prophet.data.test <- data.frame(ds = new_val$datetime_beginning_ept, y = new_val$mw)
Prof.test <- prophet()
Prof.test <- add_country_holidays(Prof, "US")
prophet.data.test <- data.frame(ds = new_val$datetime_beginning_ept, y = new_val$mw)
Prof.test <- prophet()
Prof.test <- add_country_holidays(Prof, "US")
prophet.data.test <- data.frame(ds = new_val$datetime_beginning_ept, y = new_val$mw)
Prof.test <- prophet()
Prof.test <- add_country_holidays(Prof.test, "US")
Prof.test <- add_seasonality(Prof.test, name='monthly', period=30.5, fourier.order=6)
Prof.test <- fit.prophet(Prof.test, prophet.data.test)
prof.test
forecast.data.test <- make_future_dataframe(Prof.test, periods = 168, freq = 'hour')
# Calculate prediction errors from forecast
Prophet.error.test <- test$mw - tail(predict(Prof.test, forecast.data.test)$yhat, 168)
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE.test <- mean(abs(Prophet.error))
Prophet.MAPE.test <- mean(abs(Prophet.error)/abs(test$mw))*100
Prophet.MAE.test
Prophet.MAPE.test
energy.test <- ts(new_val[,2], start = 2019, frequency = 24)
### Test
NN.Model.test <- nnetar(diff(energy.test, 24), p = 1, P = 2)
NN.Forecast.test <- forecast::forecast(NN.Model.test.test, h = 168)
NN.Forecast.test <- forecast::forecast(NN.Model.test, h = 168)
Pass.Forecast.test <- rep(NA, 168)
for(i in 1:168){
Pass.Forecast.test[i] <- energy.test[length(energy.test) - 168 + i] + NN.Forecast.test$mean[i]
}
Pass.Forecast.test <- ts(Pass.Forecast.test, start = 2022, frequency = 24)
test_ts <- ts(test[,2], start = 2022, frequency = 24)
# Calculate prediction errors from forecast
NN.error.test <- test_ts - Pass.Forecast.test
# Calculate prediction error statistics (MAE and MAPE)
NN.MAE.test <- mean(abs(NN.error.test))
NN.MAPE.test <- mean(abs(NN.error.test)/abs(test_ts))*100
NN.MAE.test
NN.MAPE.test
energy.test <- ts(new_val[,2], start = 2019, frequency = 24)
#Create the holt winter's model
HW <- hw(energy.test, seasonal = "additive", h = 168)
#Create the holt winter's model
HW <- hw(energy, seasonal = "additive", h = 168)
HW.test <- hw(energy, seasonal = "additive", h = 168)
# Calculate prediction errors from forecast
error.test=test$mw-HW.test$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE.test=mean(abs(error))
MAPE.test=mean(abs(error)/abs(test$mw))
MAE.test
MAPE.test
HW.test <- hw(energy, seasonal = "additive", h = 168)
# Calculate prediction errors from forecast
error.test=test$mw-HW.test$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE.test=mean(abs(error.test))
MAPE.test=mean(abs(error.test)/abs(test$mw))
MAE.test
MAPE.test
#Create the holt winter's model
HW <- hw(energy, seasonal = "additive", h = 168)
summary(HW)
# Calculate prediction errors from forecast
error=validation$mw-HW$mean
# Calculate prediction error statistics (MAE and MAPE)
MAE=mean(abs(error))
MAPE=mean(abs(error)/abs(validation$mw))
MAE
MAPE
