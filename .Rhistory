library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)
library(cluster)
library(factoextra)
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(MASS)
library(kableExtra)
library(partykit)
library(dbscan)
#library(knitr)
#Initial data transformation
order <- read.csv("https://github.com/sjsimmo2/DataMining-Fall/blob/master/orderData.csv?raw=true", header = TRUE)
head(order)
order$OrderSeat <- paste(order$orderNo, order$seatNo)
order$type <- rep(c('meat', 'wine', 'side'), times= (nrow(order)/3))
order_freq <- order %>% count(item) %>% arrange(desc(n))
ggplot(data = order, aes(x = reorder(item, item, function(x) - length(x)))) + geom_bar() +
theme(axis.text.x=element_text(angle = -80, hjust = 0), plot.title = element_text(hjust = 0.5)) +
labs(x = "Item", y = "Frequency", title = "Descending Order of Item Frequency")
wideorder<- order %>%
pivot_wider(names_from = type, values_from = item)
meat_list <- unique(wideorder$meat)
wine_list <- unique(wideorder$wine)
side_list <- unique(wideorder$side)
#Plotting frequencies of individual items
ggplot(data = wideorder, aes(x = reorder(meat, meat, function(x) - length(x)))) + geom_bar() +
theme(axis.text.x=element_text(angle = -60, hjust = 0), plot.title = element_text(hjust = 0.5)) +
labs(x = "Item", y = "Frequency", title = "Descending Order of Meat Frequency")
ggplot(data = wideorder, aes(x = reorder(wine, wine, function(x) - length(x)))) + geom_bar() +
theme(axis.text.x=element_text(angle = -70, hjust = 0), plot.title = element_text(hjust = 0.5)) +
labs(x = "Item", y = "Frequency", title = "Descending Order of Wine Frequency")
ggplot(data = wideorder, aes(x = reorder(side, side, function(x) - length(x)))) + geom_bar() +
theme(axis.text.x=element_text(angle = -60, hjust = 0), plot.title = element_text(hjust = 0.5)) +
labs(x = "Item", y = "Frequency", title = "Descending Order of Side Frequency")
#Making the data transactional for apriori analysis
trans.order <- as(split(order$item, order$OrderSeat), "transactions")
order.rules.meat <- trans.order %>% apriori(parameter = list(supp = 0.008, conf = 0.15, target = "rules"), appearance = list(default = "rhs", lhs = unique(wideorder$meat))) %>% sort(by = "confidence", decreasing = TRUE)
order.rules.meat.lift <- trans.order %>% apriori(parameter = list(supp = 0.008, conf = 0.15, target = "rules"), appearance = list(default = "rhs", lhs = unique(wideorder$meat))) %>% sort(by = "lift", decreasing = TRUE)
order.rules.meat.df <- as.data.frame(inspect(order.rules.meat))
order.rules.meat.df.lift <- as.data.frame(inspect(order.rules.meat.lift))
list = str_replace(str_replace(order.rules.meat.df$rhs, "\\{", ""), "\\}", "")
list.lift = str_replace(str_replace(order.rules.meat.df.lift$rhs, "\\{", ""), "\\}", "")
index = c()
index.lift = c()
if (list[i] %in% unique(wideorder$wine)) {
index = append(index, i)
}
for (i in 1:length(list)) {
if (list[i] %in% unique(wideorder$wine)) {
index = append(index, i)
}
if (list.lift[i] %in% unique(wideorder$wine)) {
index.lift = append(index.lift, i)
}
}
order.rules.meat.df <- order.rules.meat.df[index,]
order.rules.meat.df.lift <- order.rules.meat.df.lift[index.lift,]
#Ensuring only the meat with the highest confidence/lift is represented
unique.meats.df <- order.rules.meat.df[match(unique(order.rules.meat.df$lhs), order.rules.meat.df$lhs),]
unique.meats.df <- unique.meats.df[1:length(unique(wideorder$meat)),]
unique.meats.df.lift <- order.rules.meat.df.lift[match(unique(order.rules.meat.df.lift$lhs), order.rules.meat.df.lift$lhs),]
unique.meats.df.lift <- unique.meats.df.lift[1:length(unique(wideorder$meat)),]
plot(order.rules.meat.df)
plot(unique.meats.df)
plot(unique.meats.df.lift)
View(unique.meats.df)
View(unique.meats.df.lift)
library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)
library(cluster)
library(factoextra)
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(MASS)
library(kableExtra)
library(partykit)
library(dbscan)
library(ROCR)
#library(knitr)
#import data
churn <- read.csv("https://github.com/sjsimmo2/DataMining-Fall/raw/master/TelcoChurn.csv", header = TRUE)
head(churn)
#Check the variable types
str(churn)
#Check for rows with missing values and store them in a table
missing <- churn[!complete.cases(churn$TotalCharges),]
#set seed so we all get the same training, test, train data set
set.seed(1905)
#take 80/20 sample for train and test
ss <- sample(1:2,size=nrow(churn),replace=TRUE,prob=c(0.8,0.2))
train <- churn[ss==1,]
test <- churn[ss==2,]
#impute missing values with median and create missing variable
#Repeat for test and test using the training median
train$missing[is.na(train$TotalCharges)] <- 1
train$missing[is.na(train$missing)] <- 0
train$missing <- as.factor(train$missing)
train$missing[train$missing == 1]
train$TotalCharges[is.na(train$TotalCharges)] <- median(train$TotalCharges, na.rm = T)
train$missing <- as.character(train$missing)
test$missing[is.na(test$TotalCharges)] <- 1
test$missing[is.na(test$missing)] <- 0
test$missing <- as.character(test$missing)
train$TotalCharges[is.na(train$TotalCharges)] <- 0
test$TotalCharges[is.na(test$TotalCharges)] <- 0
train[!complete.cases(train),]
str(train)
#Create classification tree
class.tree = rpart(Churn ~ . - customerID, data=train, method='class',
parms = list(split='gini'))
summary(class.tree)
print(class.tree)
#Graph of the tree
rpart.plot(class.tree)
#Misclassification rate of training and test
tscores = predict(class.tree,type='class')
scores = predict(class.tree, test, type='class')
##Training misclassification rate:
sum(tscores!=train$Churn)/nrow(train)
### test data:
sum(scores!=test$Churn)/nrow(test)
#plot ROC Curve
tscores.prob <- predict(class.tree,type="prob")
pred_val <-prediction(tscores.prob[,2],train$Churn)
perf <- performance(pred_val, measure = "tpr", x.measure = "fpr")
plot(perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Classification Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
c.train <- train
c.train[sapply(c.train, is.character)] <- lapply(c.train[sapply(c.train, is.character)],
as.factor)
c.train$customerID <- as.character(c.train$customerID)
c.train$SeniorCitizen <- as.factor(c.train$SeniorCitizen)
str(c.train)
#run model
c.tree <- ctree(Churn ~ . - customerID, data=c.train)
c.tree
plot(c.tree)
#Check Misclassification rates
c.tscores = predict(c.tree,type='response')
c.scores = predict(c.tree, test, type='response')
##Training misclassification rate:
sum(c.tscores!=train$Churn)/nrow(train)
### test data:
sum(c.scores!=test$Churn)/nrow(test)
#plot ROC Curve
c.tscores.prob <- predict(c.tree,type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],train$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
#plot ROC Curve
c.tscores.prob <- predict(c.tree, test, type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],train$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
c.test <- test
c.test[sapply(c.test, is.character)] <- lapply(c.test[sapply(c.test, is.character)],
as.factor)
c.test$customerID <- as.character(c.test$customerID)
c.test$SeniorCitizen <- as.factor(c.test$SeniorCitizen)
c.tscores.prob <- predict(c.tree, test, type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],train$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
### test data:
sum(c.scores!=c.test$Churn)/nrow(c.test)
#plot ROC Curve
c.tscores.prob <- predict(c.tree, c.test, type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],train$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
c.tscores.prob <- predict(c.tree, c.test, type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],c.train$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
View(c.tscores.prob)
c.tscores.prob <- predict(c.tree, c.test, type="prob")
c.pred_val <-prediction(c.tscores.prob[,2],c.test$Churn)
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
### test data:
sum(c.scores!=c.test$Churn)/nrow(c.test)
##Training misclassification rate:
sum(c.tscores!=train$Churn)/nrow(train)
class.tree = rpart(Churn ~ . - customerID, data=train, method='class',
parms = list(split='gini'))
summary(class.tree)
print(class.tree)
#Graph of the tree
rpart.plot(class.tree)
#Misclassification rate of training and test
tscores = predict(class.tree,type='class')
scores = predict(class.tree, test, type='class')
##Training misclassification rate:
sum(tscores!=train$Churn)/nrow(train)
### test data:
sum(scores!=test$Churn)/nrow(test)
#plot ROC Curve
tscores.prob <- predict(class.tree,test,type="prob")
pred_val <-prediction(tscores.prob[,2],train$Churn)
perf <- performance(pred_val, measure = "tpr", x.measure = "fpr")
plot(perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Classification Tree",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
#Create classification tree
class.tree = rpart(Churn ~ . - customerID, data=train, method='class',
parms = list(split='gini'))
#import data
churn <- read.csv("https://github.com/sjsimmo2/DataMining-Fall/raw/master/TelcoChurn.csv", header = TRUE)
head(churn)
#Check the variable types
str(churn)
#Check for rows with missing values and store them in a table
missing <- churn[!complete.cases(churn$TotalCharges),]
#set seed so we all get the same training, test, train data set
set.seed(1905)
#take 80/20 sample for train and test
ss <- sample(1:2,size=nrow(churn),replace=TRUE,prob=c(0.8,0.2))
train <- churn[ss==1,]
test <- churn[ss==2,]
#impute missing values with median and create missing variable
#Repeat for test and test using the training median
train$missing[is.na(train$TotalCharges)] <- 1
train$missing[is.na(train$missing)] <- 0
train$missing <- as.factor(train$missing)
train$missing[train$missing == 1]
train$TotalCharges[is.na(train$TotalCharges)] <- median(train$TotalCharges, na.rm = T)
train$missing <- as.character(train$missing)
test$missing[is.na(test$TotalCharges)] <- 1
test$missing[is.na(test$missing)] <- 0
test$missing <- as.character(test$missing)
train$TotalCharges[is.na(train$TotalCharges)] <- 0
test$TotalCharges[is.na(test$TotalCharges)] <- 0
train[!complete.cases(train),]
#Create classification tree
class.tree = rpart(Churn ~ . - customerID, data=train, method='class',
parms = list(split='gini'))
#Create classification tree
class.tree = rpart(Churn ~ . - customerID, data=train, method='class',
parms = list(split='information'))
performance(c.pred_val, measure = "auc")@y.values
c.perf <- performance(c.pred_val, measure = "tpr", x.measure = "fpr")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = "AUROC = "
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = "AUROC = ",
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + performance(pred_val, measure = "auc")@y.values),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + performance(c.pred_val, measure = "auc")@y.values),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + performance(c.pred_val, measure = "auc")@y.values[1][[1]]),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
performance(c.pred_val, measure = "auc")@y.values[1]
performance(c.pred_val, measure = "auc")@y.values[[1]]
performance(c.pred_val, measure = "auc")@y.values[[1]][1]
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + performance(c.pred_val, measure = "auc")@y.values[[1]]),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
performance(c.pred_val, measure = "auc")@y.values[[1]][1]
performance(c.pred_val, measure = "auc")@y.values[[1]]
as.numeric(performance(c.pred_val, measure = "auc")@y.values[[1]])
c.auroc<-as.numeric(performance(c.pred_val, measure = "auc")@y.values[[1]])
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = print("AUROC = " + c.auroc),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
plot(c.perf, lwd = 3, col = "dodgerblue3",
main = "ROC Cruve of Recursive Partitioning Tree",
sub = paste("AUROC = 0.8288112"),
xlab = "True Positive Rate",
ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
